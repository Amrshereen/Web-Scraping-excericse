{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"================\")\n",
    "    print(\"Good to go\")\n",
    "    print(\"================\")\n",
    "\n",
    "    page_content = response.text\n",
    "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "    book_cards = soup.find_all('div') \n",
    "\n",
    "    title = soup.find(\"title\").text\n",
    "    print(\"Title:\", title)\n",
    "    print(\"\")\n",
    "\n",
    "    # Lists to store data\n",
    "    headings = []\n",
    "    paragraphs = []\n",
    "    lists = []\n",
    "\n",
    "    table_data = []\n",
    "    Table_headers = []\n",
    "\n",
    "    book_data = []\n",
    "\n",
    "    input_feild_data = []\n",
    "\n",
    "    hyperlinks = []\n",
    "    video_links = []\n",
    "    featured_products = []\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    form = soup.find('form')\n",
    "    links = soup.find_all('a')\n",
    "    videos = soup.find_all('iframe')\n",
    "    featured_product = soup.find_all('div', class_='featured-product')\n",
    "\n",
    "\n",
    "    # HEADER: Extract headings (h1, h2, ..., h6)\n",
    "    for headers in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        headings.append(headers.text.strip())\n",
    "\n",
    "    # PARAGRAPHS: Extract paragraphs (<p>)\n",
    "    for parag in soup.find_all('p'):\n",
    "        paragraphs.append(parag.text.strip())\n",
    "\n",
    "    # LISTS: Extract list items (<li>)\n",
    "    for items in soup.find_all('li'):\n",
    "        lists.append(items.text.strip())\n",
    "\n",
    "    Table_header_row = table.find_all('th')\n",
    "    for header in Table_header_row:\n",
    "        Table_headers.append(header.text.strip())\n",
    "\n",
    "    # Add headers to the table_data list\n",
    "    table_data.append(Table_headers)\n",
    "\n",
    "    # Extract all rows from the table (excluding the header)\n",
    "    rows = table.find_all('tr')[1:]  # Skip the first row which is the header\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        column_data = [column.text.strip() for column in columns]\n",
    "        table_data.append(column_data)\n",
    "\n",
    "\n",
    "    for card in book_cards:\n",
    "        # Extract the book title, price, stock availability, and button text\n",
    "        book_title = card.find(\"strong\")\n",
    "        price = card.find('p', style=\"color: green;\")  # Price is within a <p> tag with this inline style\n",
    "        stock = card.find_all('p', style=\"color: green;\")[1] if len(card.find_all('p', style=\"color: green;\")) > 1 else None\n",
    "        button_text = card.find('button')\n",
    "\n",
    "        # Only proceed if all necessary elements are found\n",
    "        if book_title and price and stock and button_text:\n",
    "            book_title_text = book_title.text.strip()\n",
    "            book_price_text = price.text.strip()\n",
    "            book_stock_text = stock.text.strip() if stock else \"Stock info not available\"\n",
    "            button_text = button_text.text.strip()\n",
    "\n",
    "            book_data.append({\n",
    "                \"Book Title\": book_title_text,\n",
    "                \"Price\": book_price_text,\n",
    "                \"Stock\": book_stock_text,\n",
    "                \"Button Text\": button_text\n",
    "            })\n",
    "\n",
    "           \n",
    "        \n",
    "        form = soup.find('form')\n",
    "    \n",
    "    if form:\n",
    "        inputs = form.find_all('input')\n",
    "        print(\"Input Fields:\")\n",
    "        for input in inputs:\n",
    "            field_name = input.get('name')\n",
    "            field_type = input.get('type')\n",
    "            placeholder = input.get('placeholder')\n",
    "\n",
    "\n",
    "        # Append the data to the list\n",
    "            input_feild_data.append({\n",
    "                \"Name\": field_name,\n",
    "                \"Type\": field_type,\n",
    "             \"Placeholder\": placeholder\n",
    "            })\n",
    "\n",
    "\n",
    "    for link in links:\n",
    "\n",
    "        link_text = link.text.strip()\n",
    "        href = link.get('href')\n",
    "\n",
    "        hyperlinks.append({\n",
    "            \"text\": link_text,\n",
    "            \"href\" : href\n",
    "        })\n",
    "\n",
    "    for video in videos:\n",
    "        src = video.get('src')  # Default to 'N/A' if src is missing\n",
    "        video_links.append({\n",
    "        \"Iframe Source\": src\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    product_cards = soup.find_all('div', class_='product-card')\n",
    "\n",
    "    for product in product_cards:\n",
    "\n",
    "        \n",
    "        product_id = product.get('data-id')\n",
    "        \n",
    "        product_name = product.find('p', class_='name').text.strip() \n",
    "        \n",
    "        product_price = product.find('p', class_='price', style='display: none;')\n",
    "        product_price = product_price.text.strip()\n",
    "\n",
    "        product_colors = product.find('p', class_='colors').text.strip()\n",
    "\n",
    "        print(f\"{product_id} , {product_name} , {product_price} , {product_colors}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Write data to CSV\n",
    "    with open(\"Extract_Text_Data.csv\", 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        \n",
    "        writer.writerow([\"Title\", title])\n",
    "        writer.writerow([])\n",
    "\n",
    "\n",
    "        for heading in headings:\n",
    "            writer.writerow([\"Header\", heading])\n",
    "        writer.writerow([])\n",
    "\n",
    "        for para in paragraphs:\n",
    "            writer.writerow([\"Paragraph\", para])\n",
    "        writer.writerow([])\n",
    "\n",
    "        for items in lists:\n",
    "            writer.writerow([\"LIst\", items])\n",
    "        writer.writerow([])\n",
    "\n",
    "        writer.writerows(table_data)\n",
    "        writer.writerow([])\n",
    "\n",
    "    print(\"Data has been saved to 'Extract_Text_Data.csv'.\")\n",
    "\n",
    "    with open(\"Product_Information.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(book_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"Data has been saved to 'Product_Information.json'.\")\n",
    "\n",
    "    with open(\"Input_feild_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(input_feild_data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(\"Data has been saved to 'Input_feild_data.json'.\")\n",
    "\n",
    "   \n",
    "\n",
    "    links_data = {\n",
    "    \"Hyperlinks\": hyperlinks,\n",
    "    \"VideoLinks\": video_links\n",
    "    }\n",
    "\n",
    "# Save the combined data to a JSON file\n",
    "    with open(\"Links.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(links_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"Data has been saved to 'Links.json'.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, save the data to a JSON file\n",
    "    with open(\"featured_products.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(featured_products, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"Data has been saved to 'featured_products.json'.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Error: Failed to fetch the page\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
